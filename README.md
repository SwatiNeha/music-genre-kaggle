# ðŸŽµ Music Genre Classification â€” Kaggle Competition

This repository contains my endâ€‘toâ€‘end pipeline for a **music genre classification** competition on Kaggle. It covers
**EDA**, **feature engineering**, **modeling**, and **final submission** choices â€” all built to be reproducible. Images
are included where helpful (see `figures/`), and results are reported for both **validation** and **Kaggle leaderboard**.

---
## Real-World Applications

Music genre classification might begin as a Kaggle challenge, but it has clear real-world impact once taken outside the competition setting.

In the streaming industry, platforms like Spotify or Apple Music could use such models to enrich their recommendation engines. By automatically tagging songs with genres, the system can curate personalized playlists and help users discover music theyâ€™d never have found otherwise. Playlists can even adapt in real time - from mellow acoustic tracks in the evening to high-energy pop for a workout.

For the music industry and marketing, genre classifiers make it possible to automatically manage massive music catalogs without relying on inconsistent human metadata. Record labels and distributors can track trends in listener behavior across genres, informing marketing campaigns and release strategies.

On the consumer side, apps can let users upload or record snippets and instantly identify the genre, recommending similar artists and tracks. Fitness and wellness apps could tap into the same models to sync playlists with activities, boosting engagement by matching the right sound to the right moment.

And in research and education, genre classification offers a benchmark problem for teaching audio feature engineering and machine learning. More broadly, the same pipeline can be adapted to related domains such as speech emotion recognition, podcast tagging, or even anomaly detection in healthcare or industrial audio.

---

## Data Overview

- **Task**: Predict the **genre** (multiâ€‘class) of a track from audio/meta features.
- **Train/Test shape**: Provided by competition (Kaggle). See EDA for quality notes.

### Missingness (from initial inspection)
**Train**: `tempo` (7,501 â€œ?â€), `duration_ms` (10,114 â€œ-1â€), `artist_name` (9,943 empty)  
**Test**: `tempo` (1,874 â€œ?â€), `duration_ms` (2,515 â€œ-1â€), `artist_name` (2,471 empty)

> These anomalies were normalized (coerced to numeric or set to NaN) and then imputed (see preprocessing).

---

## Exploratory Data Analysis (EDA)

**Key patterns:**
1) **Strong correlation:** `loudness` â†” `energy` (â‰ˆ **0.81**) â†’ redundant signal; remove one to reduce leakage/instability.  
2) **Tradeâ€‘off:** `acousticness` is **negatively** correlated with `energy` (~ âˆ’0.68) and `loudness` (~ âˆ’0.70).  
3) **Mutual information:** High for **popularity**, **acousticness**, **speechiness**; **low** for `tempo`, `time_signature`, `duration_ms`.  
4) **Skew & outliers:** `speechiness` (skew 2.30, kurtosis 3.97), `liveness` (skew 1.72), `loudness` (skew âˆ’1.34) â€” heavy tails/outliers â†’ prefer
   log/quantile/Boxâ€‘Cox over standardization alone.  
5) **Bimodality:** `instrumentalness` has two modes â†’ useful to **bin** / **cluster** (e.g., GMM) for better class separation.

**Illustrations (placeholders):**
- Correlation heatmap â†’ `figures/corr_heatmap.png`  
- Pairwise / KDE grid â†’ `figures/pairplot_kde.png`  
- Skew/outlier diagnostics â†’ `figures/boxplots_skewness.png`  
- Instrumentalness bimodality â†’ included in above figure
(These are generated in the EDA notebook.)

---

## Preprocessing & Feature Engineering

1) **Type & value fixes**
   - `time_signature`: parsed/encoded (e.g., â€œ04â€‘Aprâ€ â†’ **4/4**, â€œ03â€‘Aprâ€ â†’ **3/4**; unknown retained)
   - `tempo`: coercion â€œ?â€ â†’ NaN (numeric) 
   - `duration_ms`: â€œâˆ’1â€ â†’ NaN as invalid duration

2) **Imputation**
   - **Iterative Imputer** for numeric columns (captures feature relations vs. singleâ€‘stat fill).

3) **Dimensionality / redundancy**
   - Drop **`energy`** (highly collinear with `loudness`).

4) **Distribution fixes**
   - **Log / PowerTransformer (Boxâ€‘Cox)** for skewed features: `speechiness`, `liveness`, `loudness`.

5) **Bimodal features**
   - **GMM features** for `instrumentalness` (binary/categorical modes).
   - **Acousticness** binned via **3â€‘component GMM** â†’ low/med/high (drop raw to avoid redundancy).

6) **Filtering / selection**
   - **Mutual Information** ranking â†’ keep highâ€‘MI (e.g., `popularity`, `acousticness`, `speechiness`), drop lowâ€‘MI (`tempo`,
     `time_signature`, `duration_ms`).
   - **RFE** (with Random Forest) â†’ retain ~**15** most relevant features.

7) **Final modeling frame**
   - Drop IDs / nonâ€‘predictive text: `instance_id`, `track_name`, `track_id`, **`artist_name`** (for final CatBoost run), `energy`.
   - **Categoricals**: `key`, `mode` (encoded when needed).  
   - **ColumnTransformer**: `StandardScaler` (numeric), `OneHotEncoder` (categorical).  
   - Target `genre` encoded via `LabelEncoder`.

---

## Models & Iterations

**Baselines / classical**
- **Logistic Regression** â†’ ~**61.75%** validation.
- **KNN** â†’ ~**60%** validation.
- **SVC (RBF)** â†’ ~**65%** validation.
- **Random Forest** (tuned) â†’ **69%** validation; **65%** Kaggle.

**Boosting family**
- **GradientBoostingClassifier** â†’ **76%** validation; **65.3%** Kaggle.
- **HistGradientBoosting** â†’ **75.74%** validation.
- **CatBoostClassifier** (early try, 1000 iters) â†’ **76.89%** validation; **66.7%** Kaggle.

**Ensembles**
- **VotingClassifier** (CatBoost + GBC + HistGB, soft) â†’ **76.91%** validation; **69%** Kaggle.

**Neural network**
- **Keras/TensorFlow** (Dense + BatchNorm + Dropout, EarlyStopping + ReduceLROnPlateau) â†’ **79.4%** validation;
  **79.42%** Kaggle.

**Final submission (selected)**
- **CatBoostClassifier** (500 iters; *without* encoding `artist_name`) â†’ **81%** validation; **81.6%** Kaggle;
  **ROCâ€‘AUC 98.44%**.  Chosen for best generalization + native categorical handling + efficiency.

**Visuals (placeholders):**
- Feature importances (CatBoost) â†’ `figures/feature_importances_catboost.png`  
---

## Results Summary

| Group                  | Model                        | Val Acc. | Kaggle | Notes |
|------------------------|------------------------------|:--------:|:------:|-------|
| Baseline               | Logistic Regression          | 61.75%   |   â€“    | Simple, interpretableã€21â€ sourceã€‘ |
| Baseline               | KNN                          | 60%      |   â€“    | Distanceâ€‘basedã€21â€ sourceã€‘ |
| Baseline               | SVC (RBF)                    | 65%      |   â€“    | Nonâ€‘linear marginã€21â€ sourceã€‘ |
| Tree Ensemble          | Random Forest (tuned)        | 69%      | 65%    | Robust; FI availableã€21â€ sourceã€‘ |
| Boosting               | GradientBoostingClassifier   | 76%      | 65.3%  | Strong baselineã€21â€ sourceã€‘ |
| Boosting               | HistGradientBoosting         | 75.74%   |   â€“    | Fast/Scalableã€21â€ sourceã€‘ |
| Boosting (CatBoost v1) | CatBoost (1000 iters)        | 76.89%   | 66.7%  | Initial configã€21â€ sourceã€‘ |
| Ensemble               | Voting (CBC+GBC+HistGB)      | 76.91%   | 69%    | Soft votingã€21â€ sourceã€‘ |
| Neural Net             | Keras Dense BN+Dropout       | 79.4%    | 79.42% | EarlyStopping, LR scheduleã€21â€ sourceã€‘ |
| **Final**              | **CatBoost (500 iters)**     | **81%**  | **81.6%** | **ROCâ€‘AUC 98.44%**; no `artist_name` encodingã€21â€ sourceã€‘ |

---

## Why CatBoost in the End?

- **Top accuracy** on both validation and leaderboard.  
- **Native categorical** support (no sparse blowâ€‘ups from oneâ€‘hot).  
- **Interpretable enough** via **feature importance**; can add **SHAP** for local explanations if needed.  
- **Efficient** (500 iters) and easy to deploy.

> Notes on interpretability tradeâ€‘offs vs. KNN/Simple models are discussed in the report; SHAP/LIME can assist deployment
explanations if required.

---

## Reproducibility

- Set global **random seed** for NumPy / frameworks.  
- Notebooks/scripts included to generate **figures** and **submissions**.  
- Pipeline stages: **EDA â†’ Preprocess/Impute â†’ Featureâ€‘eng â†’ Select â†’ Train â†’ Validate â†’ Submit**.

**Repo layout**
```
music-genre-kaggle/
â”œâ”€ data/                  # train.csv / test.csv (Kaggle)
â”œâ”€ files_created          # intermediate files created during training
â”œâ”€ EDA.ipynb
â”œâ”€ Preprocessing.ipynb
â”œâ”€ Model.ipynb
â”œâ”€ figures/               # plots referenced in README
â””â”€ README.md              # this file
```

